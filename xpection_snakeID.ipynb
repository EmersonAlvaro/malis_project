{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmtHt7Kox27m"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import pandas as pd\n",
        "import os\n",
        "import pathlib\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import regularizers\n",
        "# from tensorflow.keras.utils import np_utils\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam, SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PBPmWDCyB-m",
        "outputId": "5f1ce11f-2e36-4868-99e8-245c3ddeddb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jThdj9z0x27s"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 256\n",
        "pathtxt = pathlib.Path('pathtxt/')\n",
        "data = pathlib.Path('datasetproject/')\n",
        "train_data = pathlib.Path('datasetproject/train')\n",
        "test_data = pathlib.Path('datasetproject/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un6jDvF7x27t"
      },
      "outputs": [],
      "source": [
        "def load_data(data_directory):\n",
        "    directories = [d for d in os.listdir(data_directory)\n",
        "                   if os.path.isdir(os.path.join(data_directory, d))]\n",
        "    labels = []\n",
        "    images = []\n",
        "    for d in directories:\n",
        "        label_directory = os.path.join(data_directory, d)\n",
        "        file_names = [os.path.join(label_directory, f)\n",
        "                      for f in os.listdir(label_directory)]\n",
        "        for f in file_names:\n",
        "            img = cv.imread(f)\n",
        "            img = cv.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "            images.append(img)\n",
        "            labels.append(int(d))\n",
        "\n",
        "    images, labels = np.asarray(images), np.asarray(labels)\n",
        "\n",
        "    return images, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr99Rkfdx27u"
      },
      "outputs": [],
      "source": [
        "num_folds = 10\n",
        "\n",
        "num_class = 6\n",
        "IMAGE_SIZE = 256\n",
        "epoch = 70\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "optmizer = SGD(learning_rate=0.1, momentum=0.9)\n",
        "batchsize = 32\n",
        "\n",
        "# container for metrics\n",
        "acc_folds = []\n",
        "f1_folds = []\n",
        "prec_folds = []\n",
        "recall_folds = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKaEs3fmx27v"
      },
      "outputs": [],
      "source": [
        "def xception(num_class):\n",
        "    stride = (2, 2)\n",
        "    kernel_size = (3, 3)\n",
        "    pool_size = (2, 2)\n",
        "    Channel_axis = 3\n",
        "\n",
        "    def entry_flow(img_input):\n",
        "\n",
        "        x = layers.Conv2D(32, kernel_size=kernel_size,\n",
        "                          use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                          strides=stride, padding='SAME')(img_input)\n",
        "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        x = layers.Conv2D(64, kernel_size=kernel_size,\n",
        "                          use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                          padding='SAME')(x)\n",
        "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        x_temp = x\n",
        "\n",
        "        for filter in [128, 256, 728]:\n",
        "            if filter != 128:\n",
        "                x = LeakyReLU(alpha=0.1)(x)\n",
        "            x = layers.SeparableConv2D(filters=filter, kernel_size=kernel_size,\n",
        "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                                       padding='SAME')(x)\n",
        "            x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
        "\n",
        "            x = LeakyReLU(alpha=0.1)(x)\n",
        "            x = layers.SeparableConv2D(filters=filter, kernel_size=kernel_size,\n",
        "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                                       padding='SAME')(x)\n",
        "            x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
        "\n",
        "            x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "            x_shortcut = layers.Conv2D(filters=filter, kernel_size=(1, 1),\n",
        "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                                       strides=stride, padding='SAME')(x_temp)\n",
        "            x_shortcut = layers.BatchNormalization(axis=Channel_axis)(x_shortcut)\n",
        "\n",
        "            x = layers.add([x, x_shortcut])\n",
        "            x_temp = x\n",
        "\n",
        "        return x\n",
        "\n",
        "    def middle_flow(x):\n",
        "\n",
        "        x_temp = x\n",
        "\n",
        "        for i in range(8):\n",
        "            x = LeakyReLU(alpha=0.1)(x)\n",
        "            x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n",
        "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                                       padding='SAME')(x)\n",
        "            x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
        "\n",
        "            x = LeakyReLU(alpha=0.1)(x)\n",
        "            x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n",
        "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                                       padding='SAME')(x)\n",
        "            x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
        "\n",
        "            x = LeakyReLU(alpha=0.1)(x)\n",
        "            x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n",
        "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                                       padding='SAME')(x)\n",
        "            x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
        "\n",
        "            x = layers.add([x, x_temp])\n",
        "            x_temp = x\n",
        "\n",
        "        return x\n",
        "\n",
        "    def exit_flow(x):\n",
        "\n",
        "        x_temp = x\n",
        "\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n",
        "                                   use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                                   padding='SAME')(x)\n",
        "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
        "\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "        x = layers.SeparableConv2D(filters=1024, kernel_size=kernel_size,\n",
        "                                   use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                                   padding='SAME')(x)\n",
        "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
        "\n",
        "        x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "        x_shortcut = layers.Conv2D(filters=1024, kernel_size=kernel_size,\n",
        "                                   use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                                   strides=stride, padding='SAME')(x_temp)\n",
        "        x_shortcut = layers.BatchNormalization(axis=Channel_axis)(x_shortcut)\n",
        "\n",
        "        x = layers.add([x, x_shortcut])\n",
        "\n",
        "        x = layers.SeparableConv2D(filters=1536, kernel_size=kernel_size,\n",
        "                                   use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                                   padding='SAME')(x)\n",
        "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        x = layers.SeparableConv2D(filters=2048, kernel_size=kernel_size,\n",
        "                                   use_bias=False, kernel_initializer='glorot_uniform',\n",
        "                                   padding='SAME')(x)\n",
        "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
        "        x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "        #         x = layers.Dropout(0.8)(x)\n",
        "\n",
        "        output = layers.Dense(num_class, activation=tf.nn.softmax, dtype=tf.float32)(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "    img_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "\n",
        "    x = entry_flow(img_input)\n",
        "    x = middle_flow(x)\n",
        "    output = exit_flow(x)\n",
        "\n",
        "    model = models.Model(inputs=img_input, outputs=output, name='Xception')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1irvvSnix27z",
        "outputId": "adee3966-ce65-4172-ac2f-41046a6995b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            " 4/18 [=====>........................] - ETA: 8:07 - loss: 2.9220 - accuracy: 0.2188"
          ]
        }
      ],
      "source": [
        "imagens, labels = load_data(str(train_data))\n",
        "\n",
        "# Cross fold validation\n",
        "\n",
        "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=29)\n",
        "\n",
        "for train, test in kf.split(imagens, labels):\n",
        "    imgs_train, imgs_val = imagens[train], imagens[test]\n",
        "    labs_train, labs_val = labels[train], labels[test]\n",
        "\n",
        "    model = None\n",
        "    model = xception(num_class)\n",
        "    model.compile(loss=loss_function,\n",
        "                      optimizer=optmizer,\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    xcept = model.fit(imgs_train, labs_train,\n",
        "                      batch_size=batchsize,\n",
        "                      validation_data=(imgs_val, labs_val), verbose=1,\n",
        "                      epochs=epoch)\n",
        "    \n",
        "    imgs_test, labs_test = load_data(test_data)\n",
        "\n",
        "    labs_predict = model.predict(imgs_test)\n",
        "    labs_predict = np.argmax(labs_predict, axis=1)\n",
        "\n",
        "    accuracy = metrics.accuracy_score(labs_test, labs_predict)\n",
        "    acc_folds.append(accuracy)\n",
        "\n",
        "    f1 = metrics.f1_score(labs_test, labs_predict, average=\"macro\")\n",
        "    f1_folds.append(f1)\n",
        "\n",
        "    precision = metrics.precision_score(labs_test, labs_predict, average=\"macro\")\n",
        "    prec_folds.append(precision)\n",
        "\n",
        "    recall = metrics.recall_score(labs_test, labs_predict, average=\"macro\")\n",
        "    recall_folds.append(recall)\n",
        "\n",
        "    print('Acurracy: %f' % accuracy)\n",
        "    print('F1: %f' % f1)\n",
        "    print('Precision: %f' % precision)\n",
        "    print('Recall: %f' % recall)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "xpection_snakeID.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "307064ab1b0a1923a6ed8a3693f1590945beb620e492c85a4ee4a159e00cd3fc"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('deep': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
