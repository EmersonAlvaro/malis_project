{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import regularizers\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from: \n",
    "\n",
    "https://www.aicrowd.com/challenges/snakeclef2021-snake-species-identification-challenge/dataset_files\n",
    "\n",
    "Files to be download:\n",
    "1 - SnakeCLEF-2021 - TrainingData\n",
    "2 - SnakeCLEF2021 - TrainVal Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List and id of species for this projects\n",
    "-------------------------------\n",
    "001 |Vipera berus\n",
    "002 |Vipera ursinii\n",
    "003 |Vipera seoanei\n",
    "004 |Vipera ammodytes\n",
    "005 |Vipera seoanei\n",
    "006 |Vipera xanthina\n",
    "007 |Vipera nikolskii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do List\n",
    "1 - Unzip the Train Data\n",
    "2 - Found the path to each species on TrainVal MetadatA\n",
    "3 - Create one folder for each image on our list(folder nome the ID)\n",
    "4 - Randonaly split the dataset\n",
    "Dataset Clean\n",
    "1 - Remove bad images (visual inspection)\n",
    "2 - Normalize all images of dataset (same size and dived by 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathtxt = pathlib.Path('pathtxt/')\n",
    "\n",
    "data = pathlib.Path('datasetproject/')\n",
    "train_data = pathlib.Path('datasetproject/train')\n",
    "test_data = pathlib.Path('datasetproject/test')\n",
    "\n",
    "if not os.path.exists(data):\n",
    "    os.makedirs(data)\n",
    "if not os.path.exists(train_data):\n",
    "    os.makedirs(train_data)\n",
    "if not os.path.exists(test_data):\n",
    "    os.makedirs(test_data)\n",
    "\n",
    "def creat_fold():\n",
    "\n",
    "    for filename in pathtxt.glob('*.txt'):\n",
    "        foldername = filename.name.split('.')[0]\n",
    "        print(foldername)\n",
    "        pathSpecies =train_data.joinpath(foldername)\n",
    "        pathlib.Path(pathSpecies).mkdir()\n",
    "\n",
    "        with open(filename) as datafile:\n",
    "            for i, line in enumerate(datafile.readlines()):\n",
    "                line = line.rstrip('\\n')\n",
    "                name = line.split('/')[-1]\n",
    "                shutil.copyfile(line, train_data.joinpath(foldername).joinpath(name))\n",
    "\n",
    "# creat_fold()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_ratio = 0.20\n",
    "\n",
    "def split_data():\n",
    "\n",
    "    for dr in train_data.glob('*/'):\n",
    "        pathSpecies =test_data.joinpath(dr.name)\n",
    "        pathlib.Path(pathSpecies).mkdir()\n",
    "        files = os.listdir(dr)\n",
    "\n",
    "        random.shuffle(files)\n",
    "\n",
    "        qtd = round(len(files) * split_ratio)\n",
    "        print(qtd)\n",
    "\n",
    "        for i in range(qtd):\n",
    "            src = train_data.joinpath(dr.name).joinpath(files[i])\n",
    "            dst = test_data.joinpath(dr.name).joinpath(files[i])\n",
    "            os.rename(src, dst)\n",
    "\n",
    "    return \n",
    "\n",
    "# split_data()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "307064ab1b0a1923a6ed8a3693f1590945beb620e492c85a4ee4a159e00cd3fc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('deep': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
