{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import regularizers\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from: \n",
    "\n",
    "https://www.aicrowd.com/challenges/snakeclef2021-snake-species-identification-challenge/dataset_files\n",
    "\n",
    "Files to be download:\n",
    "1 - SnakeCLEF-2021 - TrainingData\n",
    "2 - SnakeCLEF2021 - TrainVal Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List and id of species for this projects\n",
    "-------------------------------\n",
    "001 |Vipera berus\n",
    "002 |Vipera ursinii\n",
    "003 |Vipera seoanei\n",
    "004 |Vipera ammodytes\n",
    "005 |Vipera seoanei\n",
    "006 |Vipera xanthina\n",
    "007 |Vipera nikolskii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do List\n",
    "1 - Unzip the Train Data\n",
    "2 - Found the path to each species on TrainVal MetadatA\n",
    "3 - Create one folder for each image on our list(folder nome the ID)\n",
    "4 - Randonaly split the dataset\n",
    "Dataset Clean\n",
    "1 - Remove bad images (visual inspection)\n",
    "2 - Normalize all images of dataset (same size and dived by 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathtxt = pathlib.Path('pathtxt/')\n",
    "\n",
    "data = pathlib.Path('datasetproject/')\n",
    "train_data = pathlib.Path('datasetproject/train')\n",
    "test_data = pathlib.Path('datasetproject/test')\n",
    "\n",
    "if not os.path.exists(data):\n",
    "    os.makedirs(data)\n",
    "if not os.path.exists(train_data):\n",
    "    os.makedirs(train_data)\n",
    "if not os.path.exists(test_data):\n",
    "    os.makedirs(test_data)\n",
    "\n",
    "def creat_fold():\n",
    "\n",
    "    for filename in pathtxt.glob('*.txt'):\n",
    "        foldername = filename.name.split('.')[0]\n",
    "        print(foldername)\n",
    "        pathSpecies =train_data.joinpath(foldername)\n",
    "        pathlib.Path(pathSpecies).mkdir()\n",
    "\n",
    "        with open(filename) as datafile:\n",
    "            for i, line in enumerate(datafile.readlines()):\n",
    "                line = line.rstrip('\\n')\n",
    "                name = line.split('/')[-1]\n",
    "                shutil.copyfile(line, train_data.joinpath(foldername).joinpath(name))\n",
    "\n",
    "# creat_fold()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_ratio = 0.20\n",
    "\n",
    "def split_data():\n",
    "\n",
    "    for dr in train_data.glob('*/'):\n",
    "        pathSpecies =test_data.joinpath(dr.name)\n",
    "        pathlib.Path(pathSpecies).mkdir()\n",
    "        files = os.listdir(dr)\n",
    "\n",
    "        random.shuffle(files)\n",
    "\n",
    "        qtd = round(len(files) * split_ratio)\n",
    "        print(qtd)\n",
    "\n",
    "        for i in range(qtd):\n",
    "            src = train_data.joinpath(dr.name).joinpath(files[i])\n",
    "            dst = test_data.joinpath(dr.name).joinpath(files[i])\n",
    "            os.rename(src, dst)\n",
    "\n",
    "    return \n",
    "\n",
    "# split_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "\n",
    "def load_data(data_directory):\n",
    "    directories = [d for d in os.listdir(data_directory)\n",
    "                   if os.path.isdir(os.path.join(data_directory, d))]\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in directories:\n",
    "        label_directory = os.path.join(data_directory, d)\n",
    "        file_names = [os.path.join(label_directory, f)\n",
    "                      for f in os.listdir(label_directory)]\n",
    "        for f in file_names:\n",
    "            img = cv.imread(f)\n",
    "            img = cv.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            # cv.imwrite(f,img)\n",
    "            images.append(img)\n",
    "            labels.append(int(d))\n",
    "\n",
    "    images, labels = np.asarray(images), np.asarray(labels)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 6\n",
    "IMAGE_SIZE = 256\n",
    "epoch = 70\n",
    "loss_function = sparse_categorical_crossentropy\n",
    "optmizer = Adam()\n",
    "num_folds = 10\n",
    "batchsize = 32\n",
    "\n",
    "# container for metrics\n",
    "acc_folds = []\n",
    "f1_folds = []\n",
    "prec_folds = []\n",
    "recall_folds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xception(num_class):\n",
    "    stride = (2, 2)\n",
    "    kernel_size = (3, 3)\n",
    "    pool_size = (2, 2)\n",
    "    Channel_axis = 3\n",
    "\n",
    "    def entry_flow(img_input):\n",
    "\n",
    "        x = layers.Conv2D(32, kernel_size=kernel_size,\n",
    "                          use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                          strides=stride, padding='SAME')(img_input)\n",
    "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "        x = layers.Conv2D(64, kernel_size=kernel_size,\n",
    "                          use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                          padding='SAME')(x)\n",
    "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "        x_temp = x\n",
    "\n",
    "        for filter in [128, 256, 728]:\n",
    "            if filter != 128:\n",
    "                x = LeakyReLU(alpha=0.1)(x)\n",
    "            x = layers.SeparableConv2D(filters=filter, kernel_size=kernel_size,\n",
    "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                                       padding='SAME')(x)\n",
    "            x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
    "\n",
    "            x = LeakyReLU(alpha=0.1)(x)\n",
    "            x = layers.SeparableConv2D(filters=filter, kernel_size=kernel_size,\n",
    "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                                       padding='SAME')(x)\n",
    "            x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
    "\n",
    "            x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "            x_shortcut = layers.Conv2D(filters=filter, kernel_size=(1, 1),\n",
    "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                                       strides=stride, padding='SAME')(x_temp)\n",
    "            x_shortcut = layers.BatchNormalization(axis=Channel_axis)(x_shortcut)\n",
    "\n",
    "            x = layers.add([x, x_shortcut])\n",
    "            x_temp = x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def middle_flow(x):\n",
    "\n",
    "        x_temp = x\n",
    "\n",
    "        for i in range(8):\n",
    "            x = LeakyReLU(alpha=0.1)(x)\n",
    "            x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n",
    "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                                       padding='SAME')(x)\n",
    "            x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
    "\n",
    "            x = LeakyReLU(alpha=0.1)(x)\n",
    "            x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n",
    "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                                       padding='SAME')(x)\n",
    "            x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
    "\n",
    "            x = LeakyReLU(alpha=0.1)(x)\n",
    "            x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n",
    "                                       use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                                       padding='SAME')(x)\n",
    "            x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
    "\n",
    "            x = layers.add([x, x_temp])\n",
    "            x_temp = x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def exit_flow(x):\n",
    "\n",
    "        x_temp = x\n",
    "\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "        x = layers.SeparableConv2D(filters=728, kernel_size=kernel_size,\n",
    "                                   use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                                   padding='SAME')(x)\n",
    "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
    "\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "        x = layers.SeparableConv2D(filters=1024, kernel_size=kernel_size,\n",
    "                                   use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                                   padding='SAME')(x)\n",
    "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
    "\n",
    "        x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "        x_shortcut = layers.Conv2D(filters=1024, kernel_size=kernel_size,\n",
    "                                   use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                                   strides=stride, padding='SAME')(x_temp)\n",
    "        x_shortcut = layers.BatchNormalization(axis=Channel_axis)(x_shortcut)\n",
    "\n",
    "        x = layers.add([x, x_shortcut])\n",
    "\n",
    "        x = layers.SeparableConv2D(filters=1536, kernel_size=kernel_size,\n",
    "                                   use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                                   padding='SAME')(x)\n",
    "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "        x = layers.SeparableConv2D(filters=2048, kernel_size=kernel_size,\n",
    "                                   use_bias=False, kernel_initializer='glorot_uniform',\n",
    "                                   padding='SAME')(x)\n",
    "        x = layers.BatchNormalization(axis=Channel_axis)(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        #         x = layers.Dropout(0.8)(x)\n",
    "\n",
    "        output = layers.Dense(num_class, activation=tf.nn.softmax, dtype=tf.float32)(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "    img_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "    x = entry_flow(img_input)\n",
    "    x = middle_flow(x)\n",
    "    output = exit_flow(x)\n",
    "\n",
    "    model = models.Model(inputs=img_input, outputs=output, name='Xception')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 15:48:19.690772: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-25 15:48:19.707376: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-25 15:48:19.917423: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-11-25 15:48:27.645270: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-11-25 15:48:27.671647: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2112000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n"
     ]
    }
   ],
   "source": [
    "imagens, labels = load_data(str(train_data))\n",
    "\n",
    "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=29)\n",
    "\n",
    "for train, test in kf.split(imagens, labels):\n",
    "    imgs_train, imgs_val = imagens[train], imagens[test]\n",
    "    labs_train, labs_val = labels[train], labels[test]\n",
    "\n",
    "    model = None\n",
    "    model = xception(num_class)\n",
    "    model.compile(loss=loss_function,\n",
    "                      optimizer=optmizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    xcept = model.fit(imgs_train, labs_train,\n",
    "                      batch_size=batchsize,\n",
    "                      validation_data=(imgs_val, labs_val), verbose=1,\n",
    "                      epochs=epoch)\n",
    "    \n",
    "    imgs_test, labs_test = load_data(test_data)\n",
    "\n",
    "    labs_predict = model.predict(imgs_test)\n",
    "    labs_predict = np.argmax(labs_predict, axis=1)\n",
    "\n",
    "    labs_predict = model.predict(imgs_test)\n",
    "    labs_predict = np.argmax(labs_predict, axis=1)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(labs_test, labs_predict)\n",
    "    acc_folds.append(accuracy)\n",
    "\n",
    "    f1 = metrics.f1_score(labs_test, labs_predict, average=\"macro\")\n",
    "    f1_folds.append(f1)\n",
    "\n",
    "    precision = metrics.precision_score(labs_test, labs_predict, average=\"macro\")\n",
    "    prec_folds.append(precision)\n",
    "\n",
    "    recall = metrics.recall_score(labs_test, labs_predict, average=\"macro\")\n",
    "    recall_folds.append(recall)\n",
    "\n",
    "    print('Acurracy: %f' % accuracy)\n",
    "    print('F1: %f' % f1)\n",
    "    print('Precision: %f' % precision)\n",
    "    print('Recall: %f' % recall)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "307064ab1b0a1923a6ed8a3693f1590945beb620e492c85a4ee4a159e00cd3fc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('deep': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
